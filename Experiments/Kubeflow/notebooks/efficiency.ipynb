{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the use of the UCI Efficiency Dataset in a machine learning pipeline powered by Kubeflow Pipelines (KFP). The goal is to preprocess, train, and evaluate models efficiently while leveraging the power of Kubernetes-based orchestration.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Dataset: UCI Energy Efficiency Dataset\n",
    "\n",
    "The [UCI Energy Efficiency Dataset](https://archive.ics.uci.edu/dataset/242/energy+efficiency) contains features related to energy efficiency in buildings. It is ideal for exploring regression tasks such as predicting heating and cooling loads based on building properties.\n",
    "\n",
    "## Key Steps:\n",
    "\n",
    "1. **Data Ingestion:** Load the dataset directly into the notebook and perform exploratory data analysis (EDA) to understand its structure and features.\n",
    "\n",
    "\n",
    "2. **Preprocessing:** Handle missing values, normalize features, and split the data into training and testing sets.\n",
    "\n",
    "\n",
    "3. **Modeling:** Train a neural network to predict heating and cooling loads.\n",
    "\n",
    "\n",
    "4. **Evaluation:** Evaluate models using appropriate metrics such as RÂ² and Mean Squared Error (MSE).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Workflow Orchestration with Kubeflow Pipelines\n",
    "\n",
    "Kubeflow Pipelines is used to automate and orchestrate the pipeline, ensuring scalability and reproducibility. The pipeline consists of modular components for each step (data preprocessing, training, evaluation, etc.).\n",
    "\n",
    "## Tools Used:\n",
    "\n",
    "KFP Python SDK: Used to define and manage pipeline components directly within this notebook.\n",
    "\n",
    "## Pipeline Components:\n",
    "\n",
    "- Data Preprocessing: Cleans and transforms the dataset.\n",
    "- Model Training: Trains machine learning models.\n",
    "- Model Evaluation: Evaluates and validates the model performance.\n",
    "- Artifact Logging: Logs artifacts such as trained models and metrics for visualization in the Kubeflow dashboard.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## How to Run the Notebook\n",
    "\n",
    "1. Prerequisites:\n",
    "    - Install the kfp Python package.\n",
    "    - Access to a Kubeflow cluster.\n",
    "\n",
    "2. Steps:\n",
    "    - Define the pipeline steps using the KFP SDK.\n",
    "    - Compile and upload the pipeline to the Kubeflow UI.\n",
    "    - Execute the pipeline and monitor the progress via the Kubeflow dashboard.\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "**Model Artifacts:** Trained models and performance metrics.\n",
    "\n",
    "**Pipeline Logs:** Available for each step via the Kubeflow UI.\n",
    "\n",
    "**Insights:** Visualizations and metrics to interpret model performance.\n",
    "\n",
    "\n",
    "This notebook serves as an end-to-end demonstration of using the UCI Efficiency Dataset with Kubeflow Pipelines for efficient and reproducible machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade kfp==1.8.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "# For creating the pipeline\n",
    "from kfp.v2 import dsl\n",
    "\n",
    "# For building components\n",
    "from kfp.v2.dsl import component\n",
    "\n",
    "# Type annotations for the component artifacts\n",
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Model,\n",
    "    Metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"openpyxl\"],\n",
    "    output_component_file=\"pipelines/efficiency/download_data_component.yaml\"\n",
    ")\n",
    "def download_data(url: str, output_csv: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    df = pd.read_excel(url)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df.to_csv(output_csv.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"],\n",
    "    output_component_file=\"pipelines/efficiency/split_data_component.yaml\"\n",
    ")\n",
    "def split_data(input_csv: Input[Dataset], train_csv: Output[Dataset], test_csv: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = pd.read_csv(input_csv.path)\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "    train.to_csv(train_csv.path, index=False)\n",
    "    test.to_csv(test_csv.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and running a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"efficiency-pipeline\"\n",
    ")\n",
    "def download_and_split_data(url: str):\n",
    "    download_data_task = download_data(url=url)\n",
    "    split_data_task = split_data(input_csv=download_data_task.outputs['output_csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n",
    "    pipeline_func=download_and_split_data,\n",
    "    package_path='pipelines/efficiency/download_and_split_data_pipeline.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"numpy\"],\n",
    "    output_component_file=\"pipelines/efficiency/preprocess_data_component.yaml\"\n",
    ")\n",
    "def preprocess_data(\n",
    "    input_train_csv: Input[Dataset], input_test_csv: Input[Dataset],\n",
    "    output_train_x: Output[Dataset], output_test_x: Output[Dataset],\n",
    "    output_train_y: Output[Artifact], output_test_y: Output[Artifact]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "    def format_output(data):\n",
    "        y1 = data.pop('Y1')\n",
    "        y1 = np.array(y1)\n",
    "        y2 = data.pop('Y2')\n",
    "        y2 = np.array(y2)\n",
    "        return y1, y2\n",
    "\n",
    "    def norm(x, train_stats):\n",
    "        return (x- train_stats['mean']) / train_stats['std']\n",
    "\n",
    "    train = pd.read_csv(input_train_csv.path)\n",
    "    test = pd.read_csv(input_test_csv.path)\n",
    "\n",
    "    train_stats = train.describe()\n",
    "\n",
    "    # Get Y1 and Y2 as the 2 outputs and format them as np arrays\n",
    "    train_stats.pop('Y1')\n",
    "    train_stats.pop('Y2')\n",
    "    train_stats = train_stats.transpose()\n",
    "\n",
    "    train_Y = format_output(train)\n",
    "    with open(output_train_y.path, \"wb\") as file:\n",
    "        pickle.dump(train_Y, file)\n",
    "\n",
    "    test_Y = format_output(test)\n",
    "    with open(output_test_y.path, \"wb\") as file:\n",
    "        pickle.dump(test_Y, file)\n",
    "\n",
    "    # Normalise the training and test data\n",
    "    norm_train_X = norm(train, train_stats)\n",
    "    norm_test_X = norm(test, train_stats)\n",
    "\n",
    "    norm_train_X.to_csv(output_train_x.path, index=False)\n",
    "    norm_test_X.to_csv(output_test_x.path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"tensorflow\", \"pandas\"],\n",
    "    output_component_file=\"pipelines/efficiency/train_model_component.yaml\"\n",
    ")\n",
    "def train_model(input_train_x: Input[Dataset], input_train_y: Input[Artifact],\n",
    "                output_model: Output[Model], output_history: Output[Artifact]):\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "    norm_train_X = pd.read_csv(input_train_x.path)\n",
    "\n",
    "    with open(input_train_y.path, \"rb\") as file:\n",
    "        train_Y = pickle.load(file)\n",
    "\n",
    "    def model_builder(train_X):\n",
    "        # Define the layers\n",
    "        input_layer = Input(shape=(len(train_X.columns), ))\n",
    "        first_dense = Dense(units='128', activation='relu')(input_layer)\n",
    "        second_dense = Dense(units='128', activation='relu')(first_dense)\n",
    "\n",
    "        # Y1 output will be fed directly from the second dense\n",
    "        y1_output = Dense(units='1', name='y1_output')(second_dense)\n",
    "        third_dense = Dense(units='64', name='relu')(second_dense)\n",
    "\n",
    "        # Y2 output will come from the third dense\n",
    "        y2_output = Dense(units='1', name='y2_output')(third_dense)\n",
    "\n",
    "        # Define the model with input layers and list of output layers\n",
    "        model = Model(inputs=input_layer, outputs=[y1_output, y2_output])\n",
    "\n",
    "        print(model.summary())\n",
    "\n",
    "        return model\n",
    "\n",
    "    model = model_builder(norm_train_X)\n",
    "\n",
    "    # Specify the optimizer and compile the model with loss functions for both outputs\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss={'y1_output': 'mse', 'y2_output': 'mse'},\n",
    "                  metrics={\n",
    "                    'y1_output': tf.keras.metrics.RootMeanSquaredError(),\n",
    "                    'y2_output': tf.keras.metrics.RootMeanSquaredError()})\n",
    "    # Train the model for 500 epochs\n",
    "    history = model.fit(norm_train_X, train_Y, epochs=100, batch_size=10)\n",
    "    model.save(output_model.path)\n",
    "\n",
    "    with open(output_history.path, \"wb\") as file:\n",
    "        train_Y = pickle.dump(history.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"tensorflow\", \"pandas\"],\n",
    "    output_component_file=\"pipelines/efficiency/eval_model_component.yaml\"\n",
    ")\n",
    "def eval_model(input_model: Input[Model], input_history: Input[Artifact],\n",
    "               input_test_x: Input[Dataset], input_test_y: Input[Artifact],\n",
    "               MLPipeline_Metrics: Output[Metrics]):\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "\n",
    "    model = tf.keras.models.load_model(input_model.path)\n",
    "\n",
    "    norm_test_X = pd.read_csv(input_test_x.path)\n",
    "\n",
    "    with open(input_test_y.path, \"rb\") as file:\n",
    "        test_Y = pickle.load(file)\n",
    "\n",
    "    # Test the model and print loss and mse for both outputs\n",
    "    loss, Y1_loss, Y2_loss, Y1_rmse, Y2_rmse = model.evaluate(x=norm_test_X, y=test_Y)\n",
    "    print(\"Loss = {}, Y1_loss = {}, Y1_mse = {}, Y2_loss = {}, Y2_mse = {}\".format(loss, Y1_loss, Y1_rmse, Y2_loss, Y2_rmse))\n",
    "\n",
    "    MLPipeline_Metrics.log_metric(\"loss\", loss)\n",
    "    MLPipeline_Metrics.log_metric(\"Y1_loss\", Y1_loss)\n",
    "    MLPipeline_Metrics.log_metric(\"Y2_loss\", Y2_loss)\n",
    "    MLPipeline_Metrics.log_metric(\"Y1_rmse\", Y1_rmse)\n",
    "    MLPipeline_Metrics.log_metric(\"Y2_rmse\", Y2_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run the complete pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"efficiency-pipeline\",\n",
    ")\n",
    "def training_pipeline(url: str):\n",
    "\n",
    "    download_data_task = download_data(url=url)\n",
    "\n",
    "    split_data_task = split_data(input_csv=download_data_task.outputs['output_csv'])\n",
    "\n",
    "    preprocess_data_task = preprocess_data(input_train_csv=split_data_task.outputs['train_csv'],\n",
    "                                           input_test_csv=split_data_task.outputs['test_csv'])\n",
    "\n",
    "    train_model_task = train_model(input_train_x=preprocess_data_task.outputs['output_train_x'],\n",
    "                                   input_train_y=preprocess_data_task.outputs['output_train_y'])\n",
    "\n",
    "    eval_model_task = eval_model(input_model=train_model_task.outputs['output_model'],\n",
    "                                 input_history=train_model_task.outputs['output_history'],\n",
    "                                 input_test_x=preprocess_data_task.outputs['output_test_x'],\n",
    "                                 input_test_y=preprocess_data_task.outputs['output_test_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n",
    "    pipeline_func=training_pipeline,\n",
    "    package_path='pipelines/efficiency/pipeline.yaml'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
