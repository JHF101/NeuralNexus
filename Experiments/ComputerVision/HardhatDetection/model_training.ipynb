{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install transformers datasets ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL\n",
    "\n",
    "Converting the ```keremberke/hard-hat-detection``` dataset to something we can train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"keremberke/hard-hat-detection\", name=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "output_dir = 'dataset'\n",
    "image_dir = os.path.join(output_dir, 'images')\n",
    "label_dir = os.path.join(output_dir, 'labels')\n",
    "splits = ['train', 'validation', 'test']\n",
    "\n",
    "# Create directories\n",
    "for split in splits:\n",
    "    os.makedirs(os.path.join(image_dir, split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(label_dir, split), exist_ok=True)\n",
    "\n",
    "def convert_to_yolo_format(bbox, img_width, img_height):\n",
    "    x_center = (bbox[0] + bbox[2] / 2) / img_width\n",
    "    y_center = (bbox[1] + bbox[3] / 2) / img_height\n",
    "    width = bbox[2] / img_width\n",
    "    height = bbox[3] / img_height\n",
    "    return x_center, y_center, width, height\n",
    "\n",
    "def save_yolo_format(dataset, split):\n",
    "    for i, example in enumerate(dataset[split]):\n",
    "        # Save image\n",
    "        img = example['image']\n",
    "        img_filename = f\"{example['image_id']}.jpg\"\n",
    "        img.save(os.path.join(image_dir, split, img_filename))\n",
    "\n",
    "        # Save labels\n",
    "        annotations = []\n",
    "        for bbox, category_id in zip(example['objects']['bbox'], example['objects']['category']):\n",
    "            x_center, y_center, width, height = convert_to_yolo_format(\n",
    "                bbox, example['width'], example['height']\n",
    "            )\n",
    "            annotations.append(f\"{category_id} {x_center} {y_center} {width} {height}\")\n",
    "\n",
    "        label_filename = f\"{example['image_id']}.txt\"\n",
    "        with open(os.path.join(label_dir, split, label_filename), 'w') as f:\n",
    "            f.write(\"\\n\".join(annotations))\n",
    "\n",
    "# Convert and save the dataset\n",
    "for split in splits:\n",
    "    save_yolo_format(dataset, split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download specific flavour\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5n', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Repo\n",
    "!git clone https://github.com/ultralytics/yolov5\n",
    "!pip install -r yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolov5/detect.py --weights yolov5n.pt --img 640 --conf 0.25 --source dataset/images/test\n",
    "# !python yolov5/detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source dataset/images/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolov5/train.py --img 640 --batch 16 --epochs 10 --data dataset.yaml --weights yolov5n.pt --name nano_experiment\n",
    "# !python yolov5/train.py --img 640 --batch 16 --epochs 1 --data dataset.yaml --weights yolov5s.pt --name experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolov5/val.py --weights runs/train/nano_experiment/weights/best.pt --data dataset.yaml --img 640\n",
    "# !python yolov5/val.py --weights runs/train/experiment/weights/best.pt --data dataset.yaml --img 640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLO model\n",
    "# model = torch.hub.load('ultralytics/yolov5', 'custom', path='runs/train/experiment/weights/best.pt')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='runs/train/nano_experiment/weights/best.pt')\n",
    "untrained_model = torch.hub.load('ultralytics/yolov5', 'yolov5n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_frame(frame, width=None, height=None):\n",
    "    if width is None and height is None:\n",
    "        return frame\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    if width and height:\n",
    "        # Both width and height are specified\n",
    "        resized_frame = cv2.resize(frame, (width, height))\n",
    "    elif width:\n",
    "        # Only width is specified, calculate height to maintain aspect ratio\n",
    "        ratio = width / float(w)\n",
    "        resized_frame = cv2.resize(frame, (width, int(h * ratio)))\n",
    "    elif height:\n",
    "        # Only height is specified, calculate width to maintain aspect ratio\n",
    "        ratio = height / float(h)\n",
    "        resized_frame = cv2.resize(frame, (int(w * ratio), height))\n",
    "\n",
    "    return resized_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to draw detections\n",
    "def draw_detections(frame, detections, color, threshold=0.3):\n",
    "    bboxes = []\n",
    "    for _, detection in detections.iterrows():\n",
    "        x1, y1, x2, y2 = detection['xmin'], detection['ymin'], detection['xmax'], detection['ymax']\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        confidence = detection['confidence']\n",
    "\n",
    "        if confidence > threshold:\n",
    "            class_name = detection['name']\n",
    "            text = f'{class_name}: {confidence:.2f}'\n",
    "            frame = cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            bboxes.append((x1, y1, x2, y2))\n",
    "    return bboxes\n",
    "\n",
    "# Helper function to check bbox overlap\n",
    "def bbox_overlap(bbox1, bbox2):\n",
    "    x1, y1, x2, y2 = bbox1\n",
    "    x1_b, y1_b, x2_b, y2_b = bbox2\n",
    "    return not (x2 < x1_b or x2_b < x1 or y2 < y1_b or y2_b < y1)\n",
    "\n",
    "video_path = 'day_at_work.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize accumulators for timing\n",
    "total_frames = 0\n",
    "total_trained_model_time = 0.0\n",
    "total_untrained_model_time = 0.0\n",
    "total_pipeline_time = 0.0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = resize_frame(frame, width=640, height=320)\n",
    "\n",
    "    # Measure the start time of the overall pipeline\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure the start time for the trained model\n",
    "    start_trained_model = time.time()\n",
    "    trained_detections = model(frame)\n",
    "    end_trained_model = time.time()\n",
    "    trained_model_time = end_trained_model - start_trained_model\n",
    "\n",
    "    # Measure the start time for the untrained model\n",
    "    start_untrained_model = time.time()\n",
    "    untrained_detections = untrained_model(frame)\n",
    "    end_untrained_model = time.time()\n",
    "    untrained_model_time = end_untrained_model - start_untrained_model\n",
    "\n",
    "    # Filter detections for people (class 0) and clothing (class 1)\n",
    "    trained_detections = trained_detections.pandas().xyxy[0]\n",
    "    # trained_detections = trained_detections[(trained_detections['class'] == 0) | (trained_detections['class'] == 1)]\n",
    "\n",
    "    untrained_detections = untrained_detections.pandas().xyxy[0]\n",
    "    # untrained_detections = untrained_detections[(untrained_detections['class'] == 0) | (untrained_detections['class'] == 1)]\n",
    "    untrained_detections = untrained_detections[(untrained_detections['class'] == 0)]\n",
    "\n",
    "    # Draw detections and collect bounding boxes\n",
    "    hardhat_bboxes = draw_detections(frame, trained_detections, (0, 255, 0))\n",
    "    person_bboxes = draw_detections(frame, untrained_detections, (255, 0, 0), threshold=0.5)\n",
    "\n",
    "    # Check for safety status\n",
    "    safe = False\n",
    "    if person_bboxes and hardhat_bboxes:\n",
    "        safe = all(any(bbox_overlap(person_bbox, hardhat_bbox) for hardhat_bbox in hardhat_bboxes) for person_bbox in person_bboxes)\n",
    "\n",
    "    # Display safety status\n",
    "    status_text = \"Safe\" if safe else \"Unsafe\"\n",
    "    status_color = (0, 255, 0) if safe else (0, 0, 255)\n",
    "    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, status_color, 2)\n",
    "\n",
    "    # Measure the end time of the overall pipeline\n",
    "    end_time = time.time()\n",
    "    total_pipeline_time_frame = end_time - start_time\n",
    "\n",
    "    # Update accumulators\n",
    "    total_frames += 1\n",
    "    total_trained_model_time += trained_model_time\n",
    "    total_untrained_model_time += untrained_model_time\n",
    "    total_pipeline_time += total_pipeline_time_frame\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_trained_model_time = total_trained_model_time / total_frames\n",
    "    avg_untrained_model_time = total_untrained_model_time / total_frames\n",
    "    avg_pipeline_time = total_pipeline_time / total_frames\n",
    "\n",
    "    # Display benchmark results on the frame\n",
    "    cv2.putText(frame, f'Model 1 time: {trained_model_time:.2f}s', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    cv2.putText(frame, f'Model 2 time: {untrained_model_time:.2f}s', (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    cv2.putText(frame, f'Total pipeline time: {total_pipeline_time_frame:.2f}s', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    cv2.putText(frame, f'Avg Model 1 time: {avg_trained_model_time:.2f}s', (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    cv2.putText(frame, f'Avg Model 2 time: {avg_untrained_model_time:.2f}s', (10, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    cv2.putText(frame, f'Avg pipeline time: {avg_pipeline_time:.2f}s', (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('YOLO Object Detection', frame)\n",
    "\n",
    "    # Press 'q' to exit the video\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export\n",
    "\n",
    "Export the model to the desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolov5/export.py --weights runs/train/nano_experiment/weights/best.pt --include tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolov5/detect.py --weights runs/train/nano_experiment/weights/best-fp16.tflite"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
